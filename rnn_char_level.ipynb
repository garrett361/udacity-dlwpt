{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "rnn_char_level.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05C-RvCIpC8L"
      },
      "source": [
        "# Minimal PyTorch RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "M7BTK44TpC8R"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUTEmPdppw4o",
        "outputId": "86e6e5fe-09dd-46ed-cc36-8d1f2365bdcb"
      },
      "source": [
        "# Give the notebook access to the rest of your google drive files.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "# Enter the relevant foldername\n",
        "FOLDERNAME = '/content/drive/My Drive/ML/Udacity_DLWPT/recurrent-neural-networks/char-rnn/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgSOXutZwvC2"
      },
      "source": [
        "# Dataloader\n",
        "\n",
        "Create a custom dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm058RQdw_PQ"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSh3Lhsnw8x6"
      },
      "source": [
        "class TextSet(Dataset):\n",
        "    def __init__(self, txt_file_path, seq_length=64, start=0, stop=None, device='cpu'):\n",
        "        self.seq_length = seq_length\n",
        "        # Read in text\n",
        "        with open(txt_file_path, 'r') as f:\n",
        "            text = f.read()\n",
        "        if stop is None:\n",
        "            self.text = text[start:]\n",
        "        else:\n",
        "            self.text = text[start:stop]\n",
        "        # Encode\n",
        "        self.chars = tuple(set(text))\n",
        "        self.n_chars = len(self.chars)\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {char: i for i, char in self.int2char.items()}\n",
        "        self.encoded_text = np.array([self.char2int[char] for char in self.text])\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of whole independent sequences minus one.\n",
        "        # Minus one because we don't wrap data around, for simplicity\n",
        "        sequences = len(self.encoded_text) // self.seq_length - 1\n",
        "        return int(sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Target is next character\n",
        "        begin = self.seq_length * idx\n",
        "        end = begin + self.seq_length\n",
        "        data = self.encoded_text[begin:end]\n",
        "        target = self.encoded_text[begin + 1:end + 1]\n",
        "        # One-hot-encode the data. \n",
        "        data_one_hot = np.zeros((self.seq_length, self.n_chars), dtype=np.float32)\n",
        "        data_one_hot[np.arange(self.seq_length), data] = 1.\n",
        "        return torch.from_numpy(data_one_hot).to(device), torch.from_numpy(target).to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqihcZM-pC8b",
        "outputId": "a4fcf8ae-997f-43d2-a54b-909305a8a3e3"
      },
      "source": [
        "# check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71jGr9TbdPQy"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kLbVH6DApC8c"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    GG: Would like to understand how n_hidden and n_layers are chosen.  n_hidden is\n",
        "    O(sentence size), maybe? Not covered in Udacity.\n",
        "\n",
        "    - tokens is the set of all possible characters\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, file_path, n_hidden=1024, n_layers=2,\n",
        "                               drop_prob=0.2):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        \n",
        "        # creating character dictionaries, as before\n",
        "        with open(file_path, 'r') as f:\n",
        "            tokens = tuple(set(f.read()))\n",
        "        self.chars = tokens\n",
        "        self.n_chars = len(self.chars)\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the layers of the model\n",
        "        # We are one-hot encoding, so the inputs are of length of the vocab\n",
        "        self.RNN = nn.RNN(len(self.chars),\n",
        "                          self.n_hidden, \n",
        "                          self.n_layers, \n",
        "                          batch_first=True, \n",
        "                          dropout=self.drop_prob)\n",
        "        # nn.RNN dropout is only applied *between* layers, so add another.\n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "        # GG: Use CrossEntropyLoss so that we only output raw scores\n",
        "        self.fc1 = nn.Linear(self.n_hidden, len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network. \n",
        "        These inputs are x, and the hidden/cell state `hidden`.\n",
        "        \"\"\"\n",
        "                \n",
        "        ## TODO: Get the outputs and the new hidden state from the RNN\n",
        "        # Pass through RNN, dropout, and linear\n",
        "        x, hidden = self.RNN(x, hidden)\n",
        "        x = self.dropout(x)\n",
        "        # Collapsing every element in each batch to a single dimension\n",
        "        x = x.contiguous().view(-1, self.n_hidden)\n",
        "        x = self.fc1(x)\n",
        "        # Output x is of shape x.shape = (batch_size * seq_length, self.chars)\n",
        "        # giving batch_size * seq_length predictions for the next character which can be checked\n",
        "        \n",
        "        return x, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state.\n",
        "        \"\"\"\n",
        "        # Create one new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of RNN\n",
        "        # GG: This just gives us an instance of the Parameter class, which we can \n",
        "        # easily overwrite as below\n",
        "        weight = next(self.parameters())\n",
        "        # Initialize to zero and move to appropriate device\n",
        "        hidden = weight.new_zeros(self.n_layers, batch_size, self.n_hidden)\n",
        "        # GG: A little confused why we need this, since if we just don't pass in any\n",
        "        # hidden state at all, it defaults to a zero tensor, no? This this just\n",
        "        # makes later code cleaner.\n",
        "        return hidden "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PnD6U9aFnO5"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddEPiVKIpRDJ"
      },
      "source": [
        "# Helper for printing samples\n",
        "def input_to_text(tensor, net):\n",
        "    seq_length = tensor.shape[0]\n",
        "    # GG: Clean up. Nested comprehension is nearly always a bad idea.\n",
        "    text = ''.join([net.int2char[int(sum(idx * num for idx, num in enumerate(tensor[row,:])).item())] for row in range(seq_length)])\n",
        "    return text\n",
        "\n",
        "def target_to_text(tensor, net):\n",
        "    seq_length = tensor.shape[0]\n",
        "    text = ''.join([net.int2char[item.item()] for item in tensor])\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RATQEho04Z3C"
      },
      "source": [
        "# File info\n",
        "file_name = 'websters'\n",
        "file_path = FOLDERNAME + 'data/' + f'{file_name}' + '.txt'\n",
        "with open(file_path, 'r') as f:\n",
        "            text_len = len(f.read())\n",
        "# Train/Val split.\n",
        "val_frac = .1\n",
        "val_idx = int((1 - val_frac) * text_len)\n",
        "seq_length = 128\n",
        "train_set = TextSet(file_path, seq_length=seq_length, stop=val_idx, device=device)\n",
        "val_set = TextSet(file_path, seq_length=seq_length, start=val_idx, device=device)\n",
        "# Batch properties\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=True,\n",
        "                          drop_last=True)\n",
        "val_loader = DataLoader(val_set, \n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        drop_last=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTe4G93HSRwJ"
      },
      "source": [
        "Some messy testing code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH4krEH8STDh",
        "outputId": "31c95813-0854-4f26-d108-e70f3d81552f"
      },
      "source": [
        "RNN_test = CharRNN(file_path)\n",
        "RNN_test.to(device)\n",
        "print(RNN_test.char2int)\n",
        "hidden_test = RNN_test.init_hidden(batch_size=batch_size)\n",
        "print('hidden shape', hidden_test.shape)\n",
        "batches = iter(val_loader)\n",
        "x_test, y_test = next(batches)\n",
        "out_test, _ = RNN_test(x_test, hidden_test)\n",
        "print('data shape', x_test.shape)\n",
        "print('target shape', y_test.shape)\n",
        "print('output shape:', out_test.shape)\n",
        "print('loss:', F.cross_entropy(out_test, y_test.view(-1)))\n",
        "#RNN_test(, hidden_0)\n",
        "# The parameters are the weights and biases, not the hidden layer data\n",
        "for param in RNN_test.named_parameters():\n",
        "    print('param:', param[0], 'shape:', param[1].shape)\n",
        "\n",
        "# Some sample text, as a sanity check\n",
        "for example in range(2):\n",
        "    sample = x_test[example]\n",
        "    target = y_test[example]\n",
        "    print(f'Sample {example + 1}\\n',\n",
        "          input_to_text(sample, RNN_test), \n",
        "          target_to_text(target, RNN_test),\n",
        "          '\\n',\n",
        "          sep='\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'N': 1, 'ç': 2, 's': 3, 'I': 4, 'ë': 5, ';': 6, 'ì': 7, '&': 8, '\\t': 9, '¼': 10, 'G': 11, 'Þ': 12, 'x': 13, \"'\": 14, '%': 15, 'E': 16, 'v': 17, '^': 18, 'r': 19, '\\\\': 20, 'D': 21, 'S': 22, 'ý': 23, '~': 24, 'P': 25, '<': 26, '§': 27, 'ï': 28, 'H': 29, '÷': 30, 'a': 31, ')': 32, '\\n': 33, '(': 34, 'Æ': 35, 'ö': 36, '2': 37, '6': 38, 'ù': 39, 'ä': 40, 'b': 41, '¾': 42, '|': 43, 'o': 44, 'g': 45, 'w': 46, 'J': 47, '\"': 48, 'z': 49, 't': 50, 'i': 51, 'k': 52, 'è': 53, 'þ': 54, '=': 55, 'ú': 56, '1': 57, 'Ç': 58, 'A': 59, 'F': 60, 'à': 61, '£': 62, '4': 63, '{': 64, 'Q': 65, '-': 66, '`': 67, 'û': 68, 'u': 69, 'j': 70, 'p': 71, 'ð': 72, '½': 73, 'O': 74, 'l': 75, '\\ufeff': 76, '$': 77, 'K': 78, 'í': 79, 'm': 80, 'R': 81, '/': 82, 'B': 83, ']': 84, 'Ü': 85, '+': 86, 'á': 87, 'ñ': 88, '¿': 89, '*': 90, 'q': 91, ',': 92, 'ã': 93, '@': 94, 'c': 95, 'U': 96, '7': 97, '#': 98, '×': 99, '>': 100, 'ü': 101, 'ó': 102, 'é': 103, '3': 104, 'ò': 105, 'X': 106, 'd': 107, '[': 108, 'º': 109, '°': 110, ' ': 111, 'W': 112, 'Y': 113, 'î': 114, 'É': 115, 'e': 116, 'Z': 117, '_': 118, '0': 119, 'M': 120, 'V': 121, '5': 122, 'h': 123, 'y': 124, '8': 125, '9': 126, 'C': 127, ':': 128, 'ô': 129, 'å': 130, 'L': 131, '}': 132, 'T': 133, 'f': 134, 'n': 135, 'æ': 136, 'ê': 137, 'â': 138, '.': 139}\n",
            "hidden shape torch.Size([2, 128, 1024])\n",
            "data shape torch.Size([128, 128, 140])\n",
            "target shape torch.Size([128, 128])\n",
            "output shape: torch.Size([16384, 140])\n",
            "loss: tensor(4.9441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "param: RNN.weight_ih_l0 shape: torch.Size([1024, 140])\n",
            "param: RNN.weight_hh_l0 shape: torch.Size([1024, 1024])\n",
            "param: RNN.bias_ih_l0 shape: torch.Size([1024])\n",
            "param: RNN.bias_hh_l0 shape: torch.Size([1024])\n",
            "param: RNN.weight_ih_l1 shape: torch.Size([1024, 1024])\n",
            "param: RNN.weight_hh_l1 shape: torch.Size([1024, 1024])\n",
            "param: RNN.bias_ih_l1 shape: torch.Size([1024])\n",
            "param: RNN.bias_hh_l1 shape: torch.Size([1024])\n",
            "param: fc1.weight shape: torch.Size([140, 1024])\n",
            "param: fc1.bias shape: torch.Size([140])\n",
            "Sample 1\n",
            "\n",
            "s that\n",
            "of the principal clause or a general word, as be, say, mention,\n",
            "enumerate, etc., is omitted. \"Men hunt, hawk, and what no\n",
            " that\n",
            "of the principal clause or a general word, as be, say, mention,\n",
            "enumerate, etc., is omitted. \"Men hunt, hawk, and what not\n",
            "\n",
            "\n",
            "Sample 2\n",
            "\n",
            "r suit has been led.\n",
            "\n",
            "TRUMP\n",
            "Trump, v. t.\n",
            "\n",
            "Defn: To play a trump card upon; to take with a trump card; as, she\n",
            "trumped the first \n",
            " suit has been led.\n",
            "\n",
            "TRUMP\n",
            "Trump, v. t.\n",
            "\n",
            "Defn: To play a trump card upon; to take with a trump card; as, she\n",
            "trumped the first t\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcYgvmjDpC8c"
      },
      "source": [
        "## Time to train\n",
        "\n",
        "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
        "\n",
        "Below we're using an Adam optimizer and cross entropy loss since we are looking at character class scores as output. We calculate the loss and perform backpropagation, as usual!\n",
        "\n",
        "A couple of details about training: \n",
        ">* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an RNN has a hidden state that is a tuple of the hidden and cell states.\n",
        "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KUHMCVvvpC8d"
      },
      "source": [
        "def train(architecture,\n",
        "          file_path,\n",
        "          n_hidden=512,\n",
        "          n_layers=2,\n",
        "          n_epochs=10,\n",
        "          batch_size=128, \n",
        "          seq_length=128, \n",
        "          lr=1e-2,\n",
        "          clip=5,\n",
        "          val_frac=0.1,\n",
        "          print_every=100,\n",
        "          drop_prob=.5,\n",
        "          device='cpu'):\n",
        "    \"\"\"Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \"\"\"\n",
        "    net = architecture(file_path, \n",
        "                       n_hidden=n_hidden, \n",
        "                       n_layers=n_layers, \n",
        "                       drop_prob=drop_prob)\n",
        "    net.to(device)\n",
        "    net.train()\n",
        "    # total number of trainable parameters\n",
        "    total_trainable_params = sum(item.numel() for item in net.parameters() if item.requires_grad)\n",
        "    # The number of trainable parameters should be similar to text length (apparently)\n",
        "    with open(file_path, 'r') as f:\n",
        "            text_len = len(f.read())\n",
        "    print(f'Trainable Parameters: {total_trainable_params:.2E}',\n",
        "          'Text length:', f'{text_len:.3E}')\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Setting up dataloaders\n",
        "    with open(file_path, 'r') as f:\n",
        "                text_len = len(f.read())\n",
        "    # Train/Val split.\n",
        "    val_idx = int((1 - val_frac) * text_len)\n",
        "    train_set = TextSet(file_path,\n",
        "                        seq_length=seq_length, \n",
        "                        stop=val_idx, \n",
        "                        device=device)\n",
        "    val_set = TextSet(file_path,\n",
        "                      seq_length=seq_length,\n",
        "                      start=val_idx, \n",
        "                      device=device)\n",
        "    print(f'Training set size: {len(train_set):.2E}',\n",
        "          f'Validation set size: {len(val_set):.2E}',\n",
        "          sep='\\n')\n",
        "\n",
        "    train_loader = DataLoader(train_set,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True)\n",
        "    val_loader = DataLoader(val_set, \n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True,\n",
        "                            drop_last=True)\n",
        "    \n",
        "    # Training.\n",
        "    net.to(device)\n",
        "    n_chars = net.n_chars\n",
        "    training_history, validation_history = [], []\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        for idx, (inputs, targets) in enumerate(train_loader):\n",
        "            # Progress counter\n",
        "            progress_points = [int(len(train_set) * step / 10) for step in range(1, 10)]\n",
        "            if idx in progress_points:\n",
        "                print(f'Epoch {e} {int(idx / len(train_set) * 100)} percent complete')\n",
        "\n",
        "            # Truncated backprop through time. Coded so that it will work for\n",
        "            # any of {RNN, LSTM, GRU}\n",
        "            h = h.detach()\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "            loss.backward()\n",
        "            training_history.append(loss.item())\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / RNNs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "        # loss stats at end of each epoch\n",
        "        else:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, targets in val_loader:\n",
        "                val_h = val_h.detach()\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output, targets.view(-1))\n",
        "                val_losses.append(val_loss.item())\n",
        "                validation_history.append(val_loss.item())\n",
        "            \n",
        "            net.train() # reset to train mode after iterationg through validation data\n",
        "            print(f\"Epoch: {e + 1}/{n_epochs}...\",\n",
        "                  f\"Loss: {loss.item():.4f}...\",\n",
        "                  f\"Val Loss: {np.mean(val_losses):.4f}\")\n",
        "        \n",
        "    # Training ended\n",
        "    else:\n",
        "        # Plot training loss history\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "        ax1.plot(training_history, label='Training Losses')\n",
        "        ax1.legend()\n",
        "        ax2.plot(validation_history, label='Validation Losses')\n",
        "        ax2.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return net"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j5vWBiMpC8d"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes, and start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJdduLta68dr",
        "outputId": "0e107346-92ba-4865-acf0-43ece34b2a57"
      },
      "source": [
        "# Copy the file locally from Drive for speed reasons\n",
        "file_name = 'websters'\n",
        "drive_file_path = FOLDERNAME + 'data/' + f'{file_name}' + '.txt'\n",
        "!cp '{drive_file_path}' . \n",
        "!ls\n",
        "file_path = 'websters.txt'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anna.txt  drive  sample_data  websters.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KlHDYpF4pC8e",
        "outputId": "6affa980-ce63-4bf5-992d-64b3d16cdd24"
      },
      "source": [
        "# train the model\n",
        "model_params = {'n_hidden': 128,\n",
        "                'n_layers': 2,\n",
        "                'batch_size': 1024,\n",
        "                'seq_length': 128,\n",
        "                'n_epochs': 512,\n",
        "                'lr': .003,\n",
        "                'device': device,\n",
        "                'drop_prob': .1,\n",
        "                'clip': 5\n",
        "}\n",
        "start = time.perf_counter()\n",
        "trained_model = train(CharRNN,\n",
        "                      file_path,\n",
        "                      **model_params)\n",
        "training_time = time.perf_counter() - start\n",
        "print('Training_time:', training_time)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters: 8.56E+04 Text length: 2.796E+07\n",
            "Training set size: 1.97E+05\n",
            "Validation set size: 2.18E+04\n",
            "Epoch: 1/512... Loss: 2.6039... Val Loss: 2.5880\n",
            "Epoch: 2/512... Loss: 2.3236... Val Loss: 2.2935\n",
            "Epoch: 3/512... Loss: 2.1546... Val Loss: 2.1219\n",
            "Epoch: 4/512... Loss: 2.0621... Val Loss: 2.0340\n",
            "Epoch: 5/512... Loss: 2.0045... Val Loss: 1.9678\n",
            "Epoch: 6/512... Loss: 1.9639... Val Loss: 1.9200\n",
            "Epoch: 7/512... Loss: 1.9377... Val Loss: 1.8838\n",
            "Epoch: 8/512... Loss: 1.8904... Val Loss: 1.8521\n",
            "Epoch: 9/512... Loss: 1.8716... Val Loss: 1.8288\n",
            "Epoch: 10/512... Loss: 1.8495... Val Loss: 1.8214\n",
            "Epoch: 11/512... Loss: 1.8389... Val Loss: 1.7984\n",
            "Epoch: 12/512... Loss: 1.8106... Val Loss: 1.7880\n",
            "Epoch: 13/512... Loss: 1.8046... Val Loss: 1.7759\n",
            "Epoch: 14/512... Loss: 1.8088... Val Loss: 1.7669\n",
            "Epoch: 15/512... Loss: 1.7933... Val Loss: 1.7608\n",
            "Epoch: 16/512... Loss: 1.7857... Val Loss: 1.7547\n",
            "Epoch: 17/512... Loss: 1.7925... Val Loss: 1.7480\n",
            "Epoch: 18/512... Loss: 1.7813... Val Loss: 1.7409\n",
            "Epoch: 19/512... Loss: 1.7677... Val Loss: 1.7371\n",
            "Epoch: 20/512... Loss: 1.7572... Val Loss: 1.7403\n",
            "Epoch: 21/512... Loss: 1.8517... Val Loss: 1.8191\n",
            "Epoch: 22/512... Loss: 1.7679... Val Loss: 1.7382\n",
            "Epoch: 23/512... Loss: 1.7634... Val Loss: 1.7305\n",
            "Epoch: 24/512... Loss: 1.7540... Val Loss: 1.7281\n",
            "Epoch: 25/512... Loss: 1.7549... Val Loss: 1.7245\n",
            "Epoch: 26/512... Loss: 1.7621... Val Loss: 1.7228\n",
            "Epoch: 27/512... Loss: 1.7619... Val Loss: 1.7196\n",
            "Epoch: 28/512... Loss: 1.7620... Val Loss: 1.7182\n",
            "Epoch: 29/512... Loss: 1.7431... Val Loss: 1.7147\n",
            "Epoch: 30/512... Loss: 1.7691... Val Loss: 1.7429\n",
            "Epoch: 31/512... Loss: 1.7576... Val Loss: 1.7225\n",
            "Epoch: 32/512... Loss: 1.7439... Val Loss: 1.7170\n",
            "Epoch: 33/512... Loss: 1.7578... Val Loss: 1.7184\n",
            "Epoch: 34/512... Loss: 1.7437... Val Loss: 1.7139\n",
            "Epoch: 35/512... Loss: 1.7387... Val Loss: 1.7102\n",
            "Epoch: 36/512... Loss: 1.7438... Val Loss: 1.7107\n",
            "Epoch: 37/512... Loss: 1.7457... Val Loss: 1.7083\n",
            "Epoch: 38/512... Loss: 1.7379... Val Loss: 1.7065\n",
            "Epoch: 39/512... Loss: 1.7286... Val Loss: 1.7072\n",
            "Epoch: 40/512... Loss: 1.7464... Val Loss: 1.7051\n",
            "Epoch: 41/512... Loss: 1.7473... Val Loss: 1.7049\n",
            "Epoch: 42/512... Loss: 1.7346... Val Loss: 1.7030\n",
            "Epoch: 43/512... Loss: 1.7408... Val Loss: 1.7007\n",
            "Epoch: 44/512... Loss: 1.7467... Val Loss: 1.7021\n",
            "Epoch: 45/512... Loss: 1.7304... Val Loss: 1.7006\n",
            "Epoch: 46/512... Loss: 1.7345... Val Loss: 1.6995\n",
            "Epoch: 47/512... Loss: 1.7289... Val Loss: 1.6989\n",
            "Epoch: 48/512... Loss: 1.7238... Val Loss: 1.7000\n",
            "Epoch: 49/512... Loss: 1.7149... Val Loss: 1.6984\n",
            "Epoch: 50/512... Loss: 1.7248... Val Loss: 1.6954\n",
            "Epoch: 51/512... Loss: 1.7299... Val Loss: 1.6940\n",
            "Epoch: 52/512... Loss: 1.7269... Val Loss: 1.6944\n",
            "Epoch: 53/512... Loss: 1.7389... Val Loss: 1.6965\n",
            "Epoch: 54/512... Loss: 1.7270... Val Loss: 1.6925\n",
            "Epoch: 55/512... Loss: 1.7162... Val Loss: 1.6931\n",
            "Epoch: 56/512... Loss: 1.7174... Val Loss: 1.6933\n",
            "Epoch: 57/512... Loss: 1.7203... Val Loss: 1.6904\n",
            "Epoch: 58/512... Loss: 1.7227... Val Loss: 1.6939\n",
            "Epoch: 59/512... Loss: 1.7293... Val Loss: 1.6904\n",
            "Epoch: 60/512... Loss: 1.7255... Val Loss: 1.6937\n",
            "Epoch: 61/512... Loss: 1.7211... Val Loss: 1.6892\n",
            "Epoch: 62/512... Loss: 1.7206... Val Loss: 1.6900\n",
            "Epoch: 63/512... Loss: 1.7100... Val Loss: 1.6880\n",
            "Epoch: 64/512... Loss: 1.7273... Val Loss: 1.6877\n",
            "Epoch: 65/512... Loss: 1.7259... Val Loss: 1.6887\n",
            "Epoch: 66/512... Loss: 1.7099... Val Loss: 1.6866\n",
            "Epoch: 67/512... Loss: 1.7275... Val Loss: 1.6873\n",
            "Epoch: 68/512... Loss: 1.7163... Val Loss: 1.6881\n",
            "Epoch: 69/512... Loss: 1.7207... Val Loss: 1.6850\n",
            "Epoch: 70/512... Loss: 1.7189... Val Loss: 1.6866\n",
            "Epoch: 71/512... Loss: 1.7169... Val Loss: 1.6836\n",
            "Epoch: 72/512... Loss: 1.7167... Val Loss: 1.6846\n",
            "Epoch: 73/512... Loss: 1.7180... Val Loss: 1.6859\n",
            "Epoch: 74/512... Loss: 1.7101... Val Loss: 1.6853\n",
            "Epoch: 75/512... Loss: 1.7180... Val Loss: 1.6832\n",
            "Epoch: 76/512... Loss: 1.7122... Val Loss: 1.6825\n",
            "Epoch: 77/512... Loss: 1.7201... Val Loss: 1.6832\n",
            "Epoch: 78/512... Loss: 1.7017... Val Loss: 1.6828\n",
            "Epoch: 79/512... Loss: 1.7153... Val Loss: 1.6835\n",
            "Epoch: 80/512... Loss: 1.7143... Val Loss: 1.6824\n",
            "Epoch: 81/512... Loss: 1.7245... Val Loss: 1.6822\n",
            "Epoch: 82/512... Loss: 1.6934... Val Loss: 1.6826\n",
            "Epoch: 83/512... Loss: 1.7134... Val Loss: 1.6799\n",
            "Epoch: 84/512... Loss: 1.7198... Val Loss: 1.6814\n",
            "Epoch: 85/512... Loss: 1.7120... Val Loss: 1.6819\n",
            "Epoch: 86/512... Loss: 1.7184... Val Loss: 1.6798\n",
            "Epoch: 87/512... Loss: 1.7109... Val Loss: 1.6790\n",
            "Epoch: 88/512... Loss: 1.7156... Val Loss: 1.6781\n",
            "Epoch: 89/512... Loss: 1.7049... Val Loss: 1.6795\n",
            "Epoch: 90/512... Loss: 1.7005... Val Loss: 1.6809\n",
            "Epoch: 91/512... Loss: 1.7150... Val Loss: 1.6785\n",
            "Epoch: 92/512... Loss: 1.7187... Val Loss: 1.6804\n",
            "Epoch: 93/512... Loss: 1.7124... Val Loss: 1.6784\n",
            "Epoch: 94/512... Loss: 1.7011... Val Loss: 1.6771\n",
            "Epoch: 95/512... Loss: 1.7115... Val Loss: 1.6764\n",
            "Epoch: 96/512... Loss: 1.7015... Val Loss: 1.6792\n",
            "Epoch: 97/512... Loss: 1.7055... Val Loss: 1.6790\n",
            "Epoch: 98/512... Loss: 1.7109... Val Loss: 1.6760\n",
            "Epoch: 99/512... Loss: 1.7148... Val Loss: 1.6768\n",
            "Epoch: 100/512... Loss: 1.7115... Val Loss: 1.6759\n",
            "Epoch: 101/512... Loss: 1.7006... Val Loss: 1.6763\n",
            "Epoch: 102/512... Loss: 1.7059... Val Loss: 1.6767\n",
            "Epoch: 103/512... Loss: 1.7109... Val Loss: 1.6771\n",
            "Epoch: 104/512... Loss: 1.6933... Val Loss: 1.6763\n",
            "Epoch: 105/512... Loss: 1.6987... Val Loss: 1.6781\n",
            "Epoch: 106/512... Loss: 1.7048... Val Loss: 1.6754\n",
            "Epoch: 107/512... Loss: 1.7057... Val Loss: 1.6758\n",
            "Epoch: 108/512... Loss: 1.7154... Val Loss: 1.6750\n",
            "Epoch: 109/512... Loss: 1.7094... Val Loss: 1.6745\n",
            "Epoch: 110/512... Loss: 1.7082... Val Loss: 1.6768\n",
            "Epoch: 111/512... Loss: 1.7059... Val Loss: 1.6754\n",
            "Epoch: 112/512... Loss: 1.7124... Val Loss: 1.6743\n",
            "Epoch: 113/512... Loss: 1.7023... Val Loss: 1.6739\n",
            "Epoch: 114/512... Loss: 1.6957... Val Loss: 1.6771\n",
            "Epoch: 115/512... Loss: 1.7155... Val Loss: 1.6754\n",
            "Epoch: 116/512... Loss: 1.6986... Val Loss: 1.6744\n",
            "Epoch: 117/512... Loss: 1.7053... Val Loss: 1.6736\n",
            "Epoch: 118/512... Loss: 1.7033... Val Loss: 1.6751\n",
            "Epoch: 119/512... Loss: 1.7034... Val Loss: 1.6724\n",
            "Epoch: 120/512... Loss: 1.6953... Val Loss: 1.6737\n",
            "Epoch: 121/512... Loss: 1.7019... Val Loss: 1.6729\n",
            "Epoch: 122/512... Loss: 1.6993... Val Loss: 1.6743\n",
            "Epoch: 123/512... Loss: 1.6974... Val Loss: 1.6755\n",
            "Epoch: 124/512... Loss: 1.6897... Val Loss: 1.6715\n",
            "Epoch: 125/512... Loss: 1.7004... Val Loss: 1.6734\n",
            "Epoch: 126/512... Loss: 1.6925... Val Loss: 1.6726\n",
            "Epoch: 127/512... Loss: 1.7047... Val Loss: 1.6741\n",
            "Epoch: 128/512... Loss: 1.7025... Val Loss: 1.6736\n",
            "Epoch: 129/512... Loss: 1.6828... Val Loss: 1.6732\n",
            "Epoch: 130/512... Loss: 1.6994... Val Loss: 1.6733\n",
            "Epoch: 131/512... Loss: 1.6954... Val Loss: 1.6719\n",
            "Epoch: 132/512... Loss: 1.6953... Val Loss: 1.6732\n",
            "Epoch: 133/512... Loss: 1.7079... Val Loss: 1.6719\n",
            "Epoch: 134/512... Loss: 1.7044... Val Loss: 1.6734\n",
            "Epoch: 135/512... Loss: 1.7078... Val Loss: 1.6738\n",
            "Epoch: 136/512... Loss: 1.6984... Val Loss: 1.6724\n",
            "Epoch: 137/512... Loss: 1.6956... Val Loss: 1.6758\n",
            "Epoch: 138/512... Loss: 1.6989... Val Loss: 1.6712\n",
            "Epoch: 139/512... Loss: 1.7029... Val Loss: 1.6746\n",
            "Epoch: 140/512... Loss: 1.7014... Val Loss: 1.6744\n",
            "Epoch: 141/512... Loss: 1.6985... Val Loss: 1.6752\n",
            "Epoch: 142/512... Loss: 1.7036... Val Loss: 1.6718\n",
            "Epoch: 143/512... Loss: 1.6963... Val Loss: 1.6738\n",
            "Epoch: 144/512... Loss: 1.6983... Val Loss: 1.6726\n",
            "Epoch: 145/512... Loss: 1.7026... Val Loss: 1.6717\n",
            "Epoch: 146/512... Loss: 1.6951... Val Loss: 1.6725\n",
            "Epoch: 147/512... Loss: 1.7209... Val Loss: 1.6967\n",
            "Epoch: 148/512... Loss: 1.6952... Val Loss: 1.6724\n",
            "Epoch: 149/512... Loss: 1.6949... Val Loss: 1.6714\n",
            "Epoch: 150/512... Loss: 1.7045... Val Loss: 1.6720\n",
            "Epoch: 151/512... Loss: 1.6793... Val Loss: 1.6726\n",
            "Epoch: 152/512... Loss: 1.6982... Val Loss: 1.6698\n",
            "Epoch: 153/512... Loss: 1.7034... Val Loss: 1.6708\n",
            "Epoch: 154/512... Loss: 1.7110... Val Loss: 1.6721\n",
            "Epoch: 155/512... Loss: 1.6947... Val Loss: 1.6720\n",
            "Epoch: 156/512... Loss: 1.6866... Val Loss: 1.6735\n",
            "Epoch: 157/512... Loss: 1.6986... Val Loss: 1.6723\n",
            "Epoch: 158/512... Loss: 1.7015... Val Loss: 1.6728\n",
            "Epoch: 159/512... Loss: 1.7039... Val Loss: 1.6698\n",
            "Epoch: 160/512... Loss: 1.7004... Val Loss: 1.6702\n",
            "Epoch: 161/512... Loss: 1.7027... Val Loss: 1.6712\n",
            "Epoch: 162/512... Loss: 1.6955... Val Loss: 1.6700\n",
            "Epoch: 163/512... Loss: 1.7018... Val Loss: 1.6693\n",
            "Epoch: 164/512... Loss: 1.6904... Val Loss: 1.6704\n",
            "Epoch: 165/512... Loss: 1.6965... Val Loss: 1.6696\n",
            "Epoch: 166/512... Loss: 1.6975... Val Loss: 1.6697\n",
            "Epoch: 167/512... Loss: 1.6921... Val Loss: 1.6690\n",
            "Epoch: 168/512... Loss: 1.6902... Val Loss: 1.6708\n",
            "Epoch: 169/512... Loss: 1.6880... Val Loss: 1.6701\n",
            "Epoch: 170/512... Loss: 1.6997... Val Loss: 1.6695\n",
            "Epoch: 171/512... Loss: 1.6925... Val Loss: 1.6689\n",
            "Epoch: 172/512... Loss: 1.6982... Val Loss: 1.6699\n",
            "Epoch: 173/512... Loss: 1.7056... Val Loss: 1.6693\n",
            "Epoch: 174/512... Loss: 1.6953... Val Loss: 1.6695\n",
            "Epoch: 175/512... Loss: 1.7030... Val Loss: 1.6701\n",
            "Epoch: 176/512... Loss: 1.6985... Val Loss: 1.6684\n",
            "Epoch: 177/512... Loss: 1.7034... Val Loss: 1.6693\n",
            "Epoch: 178/512... Loss: 1.6862... Val Loss: 1.6717\n",
            "Epoch: 179/512... Loss: 1.6854... Val Loss: 1.6700\n",
            "Epoch: 180/512... Loss: 1.6989... Val Loss: 1.6716\n",
            "Epoch: 181/512... Loss: 1.6834... Val Loss: 1.6692\n",
            "Epoch: 182/512... Loss: 1.7012... Val Loss: 1.6712\n",
            "Epoch: 183/512... Loss: 1.6995... Val Loss: 1.6686\n",
            "Epoch: 184/512... Loss: 1.6971... Val Loss: 1.6697\n",
            "Epoch: 185/512... Loss: 1.6957... Val Loss: 1.6685\n",
            "Epoch: 186/512... Loss: 1.6985... Val Loss: 1.6698\n",
            "Epoch: 187/512... Loss: 1.6927... Val Loss: 1.6699\n",
            "Epoch: 188/512... Loss: 1.6972... Val Loss: 1.6687\n",
            "Epoch: 189/512... Loss: 1.6904... Val Loss: 1.6690\n",
            "Epoch: 190/512... Loss: 1.6900... Val Loss: 1.6706\n",
            "Epoch: 191/512... Loss: 1.7600... Val Loss: 1.7246\n",
            "Epoch: 192/512... Loss: 1.6984... Val Loss: 1.6741\n",
            "Epoch: 193/512... Loss: 1.7050... Val Loss: 1.6686\n",
            "Epoch: 194/512... Loss: 1.6953... Val Loss: 1.6685\n",
            "Epoch: 195/512... Loss: 1.6962... Val Loss: 1.6686\n",
            "Epoch: 196/512... Loss: 1.7003... Val Loss: 1.6694\n",
            "Epoch: 197/512... Loss: 1.7052... Val Loss: 1.6688\n",
            "Epoch: 198/512... Loss: 1.6816... Val Loss: 1.6684\n",
            "Epoch: 199/512... Loss: 1.6930... Val Loss: 1.6692\n",
            "Epoch: 200/512... Loss: 1.6972... Val Loss: 1.6692\n",
            "Epoch: 201/512... Loss: 1.6924... Val Loss: 1.6680\n",
            "Epoch: 202/512... Loss: 1.7031... Val Loss: 1.6690\n",
            "Epoch: 203/512... Loss: 1.6810... Val Loss: 1.6681\n",
            "Epoch: 204/512... Loss: 1.6937... Val Loss: 1.6688\n",
            "Epoch: 205/512... Loss: 1.6976... Val Loss: 1.6685\n",
            "Epoch: 206/512... Loss: 1.6869... Val Loss: 1.6697\n",
            "Epoch: 207/512... Loss: 1.6935... Val Loss: 1.6687\n",
            "Epoch: 208/512... Loss: 1.6976... Val Loss: 1.6671\n",
            "Epoch: 209/512... Loss: 1.6851... Val Loss: 1.6691\n",
            "Epoch: 210/512... Loss: 1.6959... Val Loss: 1.6677\n",
            "Epoch: 211/512... Loss: 1.6974... Val Loss: 1.6689\n",
            "Epoch: 212/512... Loss: 1.6937... Val Loss: 1.6671\n",
            "Epoch: 213/512... Loss: 1.6975... Val Loss: 1.6690\n",
            "Epoch: 214/512... Loss: 1.7005... Val Loss: 1.6691\n",
            "Epoch: 215/512... Loss: 1.6991... Val Loss: 1.6667\n",
            "Epoch: 216/512... Loss: 1.6909... Val Loss: 1.6683\n",
            "Epoch: 217/512... Loss: 1.7073... Val Loss: 1.6689\n",
            "Epoch: 218/512... Loss: 1.6927... Val Loss: 1.6735\n",
            "Epoch: 219/512... Loss: 1.6936... Val Loss: 1.6694\n",
            "Epoch: 220/512... Loss: 1.6839... Val Loss: 1.6673\n",
            "Epoch: 221/512... Loss: 1.7081... Val Loss: 1.6737\n",
            "Epoch: 222/512... Loss: 1.6883... Val Loss: 1.6674\n",
            "Epoch: 223/512... Loss: 1.6890... Val Loss: 1.6670\n",
            "Epoch: 224/512... Loss: 1.6854... Val Loss: 1.6672\n",
            "Epoch: 225/512... Loss: 1.7136... Val Loss: 1.6889\n",
            "Epoch: 226/512... Loss: 1.6952... Val Loss: 1.6757\n",
            "Epoch: 227/512... Loss: 1.6912... Val Loss: 1.6730\n",
            "Epoch: 228/512... Loss: 1.6982... Val Loss: 1.6694\n",
            "Epoch: 229/512... Loss: 1.7080... Val Loss: 1.6682\n",
            "Epoch: 230/512... Loss: 1.7045... Val Loss: 1.6688\n",
            "Epoch: 231/512... Loss: 1.6956... Val Loss: 1.6707\n",
            "Epoch: 232/512... Loss: 1.6953... Val Loss: 1.6680\n",
            "Epoch: 233/512... Loss: 1.7561... Val Loss: 1.7194\n",
            "Epoch: 234/512... Loss: 1.6868... Val Loss: 1.6677\n",
            "Epoch: 235/512... Loss: 1.6803... Val Loss: 1.6679\n",
            "Epoch: 236/512... Loss: 1.6903... Val Loss: 1.6664\n",
            "Epoch: 237/512... Loss: 1.6877... Val Loss: 1.6670\n",
            "Epoch: 238/512... Loss: 1.6935... Val Loss: 1.6663\n",
            "Epoch: 239/512... Loss: 1.6837... Val Loss: 1.6654\n",
            "Epoch: 240/512... Loss: 1.6959... Val Loss: 1.6674\n",
            "Epoch: 241/512... Loss: 1.6863... Val Loss: 1.6674\n",
            "Epoch: 242/512... Loss: 1.6760... Val Loss: 1.6686\n",
            "Epoch: 243/512... Loss: 1.6981... Val Loss: 1.6673\n",
            "Epoch: 244/512... Loss: 1.6941... Val Loss: 1.6660\n",
            "Epoch: 245/512... Loss: 1.6906... Val Loss: 1.6691\n",
            "Epoch: 246/512... Loss: 1.6942... Val Loss: 1.6677\n",
            "Epoch: 247/512... Loss: 1.6958... Val Loss: 1.6675\n",
            "Epoch: 248/512... Loss: 1.6801... Val Loss: 1.6655\n",
            "Epoch: 249/512... Loss: 1.6978... Val Loss: 1.6661\n",
            "Epoch: 250/512... Loss: 1.6850... Val Loss: 1.6658\n",
            "Epoch: 251/512... Loss: 1.6901... Val Loss: 1.6664\n",
            "Epoch: 252/512... Loss: 1.7012... Val Loss: 1.6696\n",
            "Epoch: 253/512... Loss: 1.6812... Val Loss: 1.6650\n",
            "Epoch: 254/512... Loss: 1.6851... Val Loss: 1.6658\n",
            "Epoch: 255/512... Loss: 1.6959... Val Loss: 1.6664\n",
            "Epoch: 256/512... Loss: 1.6853... Val Loss: 1.6649\n",
            "Epoch: 257/512... Loss: 1.6844... Val Loss: 1.6663\n",
            "Epoch: 258/512... Loss: 1.6875... Val Loss: 1.6676\n",
            "Epoch: 259/512... Loss: 1.7018... Val Loss: 1.6739\n",
            "Epoch: 260/512... Loss: 1.6926... Val Loss: 1.6653\n",
            "Epoch: 261/512... Loss: 1.7027... Val Loss: 1.6652\n",
            "Epoch: 262/512... Loss: 1.6820... Val Loss: 1.6654\n",
            "Epoch: 263/512... Loss: 1.6934... Val Loss: 1.6654\n",
            "Epoch: 264/512... Loss: 1.6826... Val Loss: 1.6652\n",
            "Epoch: 265/512... Loss: 1.6972... Val Loss: 1.6649\n",
            "Epoch: 266/512... Loss: 1.6831... Val Loss: 1.6654\n",
            "Epoch: 267/512... Loss: 1.6920... Val Loss: 1.6650\n",
            "Epoch: 268/512... Loss: 1.6833... Val Loss: 1.6650\n",
            "Epoch: 269/512... Loss: 1.6810... Val Loss: 1.6662\n",
            "Epoch: 270/512... Loss: 1.6854... Val Loss: 1.6644\n",
            "Epoch: 271/512... Loss: 1.6826... Val Loss: 1.6651\n",
            "Epoch: 272/512... Loss: 1.6957... Val Loss: 1.6629\n",
            "Epoch: 273/512... Loss: 1.6774... Val Loss: 1.6646\n",
            "Epoch: 274/512... Loss: 1.6951... Val Loss: 1.6654\n",
            "Epoch: 275/512... Loss: 1.6953... Val Loss: 1.6652\n",
            "Epoch: 276/512... Loss: 1.6840... Val Loss: 1.6659\n",
            "Epoch: 277/512... Loss: 1.6839... Val Loss: 1.6628\n",
            "Epoch: 278/512... Loss: 1.6911... Val Loss: 1.6654\n",
            "Epoch: 279/512... Loss: 1.6773... Val Loss: 1.6647\n",
            "Epoch: 280/512... Loss: 1.6862... Val Loss: 1.6649\n",
            "Epoch: 281/512... Loss: 1.6910... Val Loss: 1.6635\n",
            "Epoch: 282/512... Loss: 1.6725... Val Loss: 1.6647\n",
            "Epoch: 283/512... Loss: 1.6947... Val Loss: 1.6646\n",
            "Epoch: 284/512... Loss: 1.6849... Val Loss: 1.6647\n",
            "Epoch: 285/512... Loss: 1.6925... Val Loss: 1.6654\n",
            "Epoch: 286/512... Loss: 1.6969... Val Loss: 1.6640\n",
            "Epoch: 287/512... Loss: 1.6856... Val Loss: 1.6640\n",
            "Epoch: 288/512... Loss: 1.6965... Val Loss: 1.6646\n",
            "Epoch: 289/512... Loss: 1.6871... Val Loss: 1.6646\n",
            "Epoch: 290/512... Loss: 1.6891... Val Loss: 1.6633\n",
            "Epoch: 291/512... Loss: 1.6936... Val Loss: 1.6649\n",
            "Epoch: 292/512... Loss: 1.6812... Val Loss: 1.6639\n",
            "Epoch: 293/512... Loss: 1.6912... Val Loss: 1.6646\n",
            "Epoch: 294/512... Loss: 1.6901... Val Loss: 1.6636\n",
            "Epoch: 295/512... Loss: 1.6882... Val Loss: 1.6654\n",
            "Epoch: 296/512... Loss: 1.6915... Val Loss: 1.6644\n",
            "Epoch: 297/512... Loss: 1.6855... Val Loss: 1.6651\n",
            "Epoch: 298/512... Loss: 1.6894... Val Loss: 1.6632\n",
            "Epoch: 299/512... Loss: 1.6819... Val Loss: 1.6635\n",
            "Epoch: 300/512... Loss: 1.6910... Val Loss: 1.6628\n",
            "Epoch: 301/512... Loss: 1.6942... Val Loss: 1.6639\n",
            "Epoch: 302/512... Loss: 1.7210... Val Loss: 1.6881\n",
            "Epoch: 303/512... Loss: 1.7041... Val Loss: 1.6741\n",
            "Epoch: 304/512... Loss: 1.7055... Val Loss: 1.6709\n",
            "Epoch: 305/512... Loss: 1.6905... Val Loss: 1.6687\n",
            "Epoch: 306/512... Loss: 1.6951... Val Loss: 1.6639\n",
            "Epoch: 307/512... Loss: 1.6754... Val Loss: 1.6639\n",
            "Epoch: 308/512... Loss: 1.6900... Val Loss: 1.6632\n",
            "Epoch: 309/512... Loss: 1.6992... Val Loss: 1.6658\n",
            "Epoch: 310/512... Loss: 1.7016... Val Loss: 1.6635\n",
            "Epoch: 311/512... Loss: 1.6979... Val Loss: 1.6639\n",
            "Epoch: 312/512... Loss: 1.6890... Val Loss: 1.6633\n",
            "Epoch: 313/512... Loss: 1.6916... Val Loss: 1.6647\n",
            "Epoch: 314/512... Loss: 1.6881... Val Loss: 1.6637\n",
            "Epoch: 315/512... Loss: 1.6750... Val Loss: 1.6635\n",
            "Epoch: 316/512... Loss: 1.6954... Val Loss: 1.6640\n",
            "Epoch: 317/512... Loss: 1.6914... Val Loss: 1.6620\n",
            "Epoch: 318/512... Loss: 1.6865... Val Loss: 1.6647\n",
            "Epoch: 319/512... Loss: 1.6933... Val Loss: 1.6633\n",
            "Epoch: 320/512... Loss: 1.6939... Val Loss: 1.6627\n",
            "Epoch: 321/512... Loss: 1.6768... Val Loss: 1.6659\n",
            "Epoch: 322/512... Loss: 1.6855... Val Loss: 1.6631\n",
            "Epoch: 323/512... Loss: 1.6810... Val Loss: 1.6634\n",
            "Epoch: 324/512... Loss: 1.6784... Val Loss: 1.6630\n",
            "Epoch: 325/512... Loss: 1.6939... Val Loss: 1.6612\n",
            "Epoch: 326/512... Loss: 1.6816... Val Loss: 1.6638\n",
            "Epoch: 327/512... Loss: 1.6798... Val Loss: 1.6629\n",
            "Epoch: 328/512... Loss: 1.6936... Val Loss: 1.6637\n",
            "Epoch: 329/512... Loss: 1.6898... Val Loss: 1.6639\n",
            "Epoch: 330/512... Loss: 1.6912... Val Loss: 1.6629\n",
            "Epoch: 331/512... Loss: 1.6834... Val Loss: 1.6639\n",
            "Epoch: 332/512... Loss: 1.6824... Val Loss: 1.6631\n",
            "Epoch: 333/512... Loss: 1.6914... Val Loss: 1.6616\n",
            "Epoch: 334/512... Loss: 1.6871... Val Loss: 1.6612\n",
            "Epoch: 335/512... Loss: 1.6850... Val Loss: 1.6625\n",
            "Epoch: 336/512... Loss: 1.6901... Val Loss: 1.6629\n",
            "Epoch: 337/512... Loss: 1.6840... Val Loss: 1.6632\n",
            "Epoch: 338/512... Loss: 1.6824... Val Loss: 1.6596\n",
            "Epoch: 339/512... Loss: 1.6892... Val Loss: 1.6658\n",
            "Epoch: 340/512... Loss: 1.6788... Val Loss: 1.6615\n",
            "Epoch: 341/512... Loss: 1.6854... Val Loss: 1.6634\n",
            "Epoch: 342/512... Loss: 1.6787... Val Loss: 1.6631\n",
            "Epoch: 343/512... Loss: 1.6841... Val Loss: 1.6626\n",
            "Epoch: 344/512... Loss: 1.6785... Val Loss: 1.6632\n",
            "Epoch: 345/512... Loss: 1.6938... Val Loss: 1.6635\n",
            "Epoch: 346/512... Loss: 1.6769... Val Loss: 1.6624\n",
            "Epoch: 347/512... Loss: 1.6653... Val Loss: 1.6618\n",
            "Epoch: 348/512... Loss: 1.6887... Val Loss: 1.6606\n",
            "Epoch: 349/512... Loss: 1.6969... Val Loss: 1.6607\n",
            "Epoch: 350/512... Loss: 1.6791... Val Loss: 1.6625\n",
            "Epoch: 351/512... Loss: 1.6851... Val Loss: 1.6618\n",
            "Epoch: 352/512... Loss: 1.6849... Val Loss: 1.6638\n",
            "Epoch: 353/512... Loss: 1.6855... Val Loss: 1.6619\n",
            "Epoch: 354/512... Loss: 1.6903... Val Loss: 1.6647\n",
            "Epoch: 355/512... Loss: 1.6763... Val Loss: 1.6603\n",
            "Epoch: 356/512... Loss: 1.6996... Val Loss: 1.6619\n",
            "Epoch: 357/512... Loss: 1.6903... Val Loss: 1.6629\n",
            "Epoch: 358/512... Loss: 1.6942... Val Loss: 1.6619\n",
            "Epoch: 359/512... Loss: 1.6835... Val Loss: 1.6611\n",
            "Epoch: 360/512... Loss: 1.6860... Val Loss: 1.6628\n",
            "Epoch: 361/512... Loss: 1.6911... Val Loss: 1.6605\n",
            "Epoch: 362/512... Loss: 1.6780... Val Loss: 1.6637\n",
            "Epoch: 363/512... Loss: 1.6821... Val Loss: 1.6621\n",
            "Epoch: 364/512... Loss: 1.6729... Val Loss: 1.6618\n",
            "Epoch: 365/512... Loss: 1.6806... Val Loss: 1.6605\n",
            "Epoch: 366/512... Loss: 1.6859... Val Loss: 1.6613\n",
            "Epoch: 367/512... Loss: 1.6770... Val Loss: 1.6614\n",
            "Epoch: 368/512... Loss: 1.6812... Val Loss: 1.6626\n",
            "Epoch: 369/512... Loss: 1.7092... Val Loss: 1.6804\n",
            "Epoch: 370/512... Loss: 1.6911... Val Loss: 1.6624\n",
            "Epoch: 371/512... Loss: 1.6766... Val Loss: 1.6605\n",
            "Epoch: 372/512... Loss: 1.6856... Val Loss: 1.6611\n",
            "Epoch: 373/512... Loss: 1.6856... Val Loss: 1.6609\n",
            "Epoch: 374/512... Loss: 1.6809... Val Loss: 1.6616\n",
            "Epoch: 375/512... Loss: 1.6762... Val Loss: 1.6621\n",
            "Epoch: 376/512... Loss: 1.6842... Val Loss: 1.6629\n",
            "Epoch: 377/512... Loss: 1.6793... Val Loss: 1.6600\n",
            "Epoch: 378/512... Loss: 1.6959... Val Loss: 1.6612\n",
            "Epoch: 379/512... Loss: 1.6767... Val Loss: 1.6602\n",
            "Epoch: 380/512... Loss: 1.6919... Val Loss: 1.6591\n",
            "Epoch: 381/512... Loss: 1.6839... Val Loss: 1.6624\n",
            "Epoch: 382/512... Loss: 1.6756... Val Loss: 1.6620\n",
            "Epoch: 383/512... Loss: 1.7242... Val Loss: 1.6935\n",
            "Epoch: 384/512... Loss: 1.6881... Val Loss: 1.6617\n",
            "Epoch: 385/512... Loss: 1.6836... Val Loss: 1.6594\n",
            "Epoch: 386/512... Loss: 1.6883... Val Loss: 1.6606\n",
            "Epoch: 387/512... Loss: 1.6913... Val Loss: 1.6612\n",
            "Epoch: 388/512... Loss: 1.6781... Val Loss: 1.6635\n",
            "Epoch: 389/512... Loss: 1.6840... Val Loss: 1.6636\n",
            "Epoch: 390/512... Loss: 1.6884... Val Loss: 1.6623\n",
            "Epoch: 391/512... Loss: 1.6878... Val Loss: 1.6614\n",
            "Epoch: 392/512... Loss: 1.6859... Val Loss: 1.6615\n",
            "Epoch: 393/512... Loss: 1.6795... Val Loss: 1.6626\n",
            "Epoch: 394/512... Loss: 1.6804... Val Loss: 1.6611\n",
            "Epoch: 395/512... Loss: 1.6842... Val Loss: 1.6625\n",
            "Epoch: 396/512... Loss: 1.6855... Val Loss: 1.6619\n",
            "Epoch: 397/512... Loss: 1.6909... Val Loss: 1.6616\n",
            "Epoch: 398/512... Loss: 1.6876... Val Loss: 1.6603\n",
            "Epoch: 399/512... Loss: 1.6809... Val Loss: 1.6618\n",
            "Epoch: 400/512... Loss: 1.6779... Val Loss: 1.6614\n",
            "Epoch: 401/512... Loss: 1.6851... Val Loss: 1.6610\n",
            "Epoch: 402/512... Loss: 1.7364... Val Loss: 1.7098\n",
            "Epoch: 403/512... Loss: 1.6943... Val Loss: 1.6624\n",
            "Epoch: 404/512... Loss: 1.6806... Val Loss: 1.6627\n",
            "Epoch: 405/512... Loss: 1.6680... Val Loss: 1.6613\n",
            "Epoch: 406/512... Loss: 1.6836... Val Loss: 1.6614\n",
            "Epoch: 407/512... Loss: 1.6919... Val Loss: 1.6608\n",
            "Epoch: 408/512... Loss: 1.6942... Val Loss: 1.6596\n",
            "Epoch: 409/512... Loss: 1.6778... Val Loss: 1.6603\n",
            "Epoch: 410/512... Loss: 1.6886... Val Loss: 1.6597\n",
            "Epoch: 411/512... Loss: 1.6751... Val Loss: 1.6601\n",
            "Epoch: 412/512... Loss: 1.6835... Val Loss: 1.6688\n",
            "Epoch: 413/512... Loss: 1.6809... Val Loss: 1.6603\n",
            "Epoch: 414/512... Loss: 1.6803... Val Loss: 1.6601\n",
            "Epoch: 415/512... Loss: 1.6851... Val Loss: 1.6643\n",
            "Epoch: 416/512... Loss: 1.6939... Val Loss: 1.6601\n",
            "Epoch: 417/512... Loss: 1.6821... Val Loss: 1.6604\n",
            "Epoch: 418/512... Loss: 1.7107... Val Loss: 1.6844\n",
            "Epoch: 419/512... Loss: 1.6890... Val Loss: 1.6640\n",
            "Epoch: 420/512... Loss: 1.6767... Val Loss: 1.6676\n",
            "Epoch: 421/512... Loss: 1.6719... Val Loss: 1.6632\n",
            "Epoch: 422/512... Loss: 1.6866... Val Loss: 1.6616\n",
            "Epoch: 423/512... Loss: 1.6821... Val Loss: 1.6617\n",
            "Epoch: 424/512... Loss: 1.6924... Val Loss: 1.6602\n",
            "Epoch: 425/512... Loss: 1.6682... Val Loss: 1.6611\n",
            "Epoch: 426/512... Loss: 1.6862... Val Loss: 1.6606\n",
            "Epoch: 427/512... Loss: 1.6914... Val Loss: 1.6606\n",
            "Epoch: 428/512... Loss: 1.6878... Val Loss: 1.6762\n",
            "Epoch: 429/512... Loss: 1.6796... Val Loss: 1.6629\n",
            "Epoch: 430/512... Loss: 1.6766... Val Loss: 1.6604\n",
            "Epoch: 431/512... Loss: 1.6843... Val Loss: 1.6608\n",
            "Epoch: 432/512... Loss: 1.6866... Val Loss: 1.6623\n",
            "Epoch: 433/512... Loss: 1.6992... Val Loss: 1.6621\n",
            "Epoch: 434/512... Loss: 1.6834... Val Loss: 1.6620\n",
            "Epoch: 435/512... Loss: 1.6827... Val Loss: 1.6602\n",
            "Epoch: 436/512... Loss: 1.6884... Val Loss: 1.6608\n",
            "Epoch: 437/512... Loss: 1.6888... Val Loss: 1.6598\n",
            "Epoch: 438/512... Loss: 1.6789... Val Loss: 1.6644\n",
            "Epoch: 439/512... Loss: 1.6789... Val Loss: 1.6595\n",
            "Epoch: 440/512... Loss: 1.6787... Val Loss: 1.6605\n",
            "Epoch: 441/512... Loss: 1.6833... Val Loss: 1.6608\n",
            "Epoch: 442/512... Loss: 1.6771... Val Loss: 1.6600\n",
            "Epoch: 443/512... Loss: 1.6834... Val Loss: 1.6611\n",
            "Epoch: 444/512... Loss: 1.6941... Val Loss: 1.6614\n",
            "Epoch: 445/512... Loss: 1.6669... Val Loss: 1.6593\n",
            "Epoch: 446/512... Loss: 1.6849... Val Loss: 1.6605\n",
            "Epoch: 447/512... Loss: 1.6786... Val Loss: 1.6586\n",
            "Epoch: 448/512... Loss: 1.6841... Val Loss: 1.6599\n",
            "Epoch: 449/512... Loss: 1.6743... Val Loss: 1.6600\n",
            "Epoch: 450/512... Loss: 1.6759... Val Loss: 1.6604\n",
            "Epoch: 451/512... Loss: 1.6805... Val Loss: 1.6609\n",
            "Epoch: 452/512... Loss: 1.6769... Val Loss: 1.6599\n",
            "Epoch: 453/512... Loss: 1.6830... Val Loss: 1.6608\n",
            "Epoch: 454/512... Loss: 1.6854... Val Loss: 1.6634\n",
            "Epoch: 455/512... Loss: 1.6874... Val Loss: 1.6616\n",
            "Epoch: 456/512... Loss: 1.6837... Val Loss: 1.6592\n",
            "Epoch: 457/512... Loss: 1.6813... Val Loss: 1.6611\n",
            "Epoch: 458/512... Loss: 1.6818... Val Loss: 1.6598\n",
            "Epoch: 459/512... Loss: 1.7022... Val Loss: 1.6694\n",
            "Epoch: 460/512... Loss: 1.6857... Val Loss: 1.6657\n",
            "Epoch: 461/512... Loss: 1.6938... Val Loss: 1.6598\n",
            "Epoch: 462/512... Loss: 1.6808... Val Loss: 1.6600\n",
            "Epoch: 463/512... Loss: 1.6967... Val Loss: 1.6651\n",
            "Epoch: 464/512... Loss: 1.6824... Val Loss: 1.6596\n",
            "Epoch: 465/512... Loss: 1.6891... Val Loss: 1.6597\n",
            "Epoch: 466/512... Loss: 1.6871... Val Loss: 1.6587\n",
            "Epoch: 467/512... Loss: 1.6884... Val Loss: 1.6605\n",
            "Epoch: 468/512... Loss: 1.6854... Val Loss: 1.6588\n",
            "Epoch: 469/512... Loss: 1.6932... Val Loss: 1.6601\n",
            "Epoch: 470/512... Loss: 1.6915... Val Loss: 1.6640\n",
            "Epoch: 471/512... Loss: 1.6934... Val Loss: 1.6587\n",
            "Epoch: 472/512... Loss: 1.6825... Val Loss: 1.6609\n",
            "Epoch: 473/512... Loss: 1.6931... Val Loss: 1.6597\n",
            "Epoch: 474/512... Loss: 1.6881... Val Loss: 1.6615\n",
            "Epoch: 475/512... Loss: 1.6805... Val Loss: 1.6601\n",
            "Epoch: 476/512... Loss: 1.6828... Val Loss: 1.6605\n",
            "Epoch: 477/512... Loss: 1.6930... Val Loss: 1.6580\n",
            "Epoch: 478/512... Loss: 1.6886... Val Loss: 1.6589\n",
            "Epoch: 479/512... Loss: 1.6806... Val Loss: 1.6605\n",
            "Epoch: 480/512... Loss: 1.6789... Val Loss: 1.6577\n",
            "Epoch: 481/512... Loss: 1.6864... Val Loss: 1.6598\n",
            "Epoch: 482/512... Loss: 1.6788... Val Loss: 1.6624\n",
            "Epoch: 483/512... Loss: 1.6843... Val Loss: 1.6592\n",
            "Epoch: 484/512... Loss: 1.6990... Val Loss: 1.6605\n",
            "Epoch: 485/512... Loss: 1.6917... Val Loss: 1.6597\n",
            "Epoch: 486/512... Loss: 1.6936... Val Loss: 1.6584\n",
            "Epoch: 487/512... Loss: 1.6810... Val Loss: 1.6600\n",
            "Epoch: 488/512... Loss: 1.6866... Val Loss: 1.6607\n",
            "Epoch: 489/512... Loss: 1.6834... Val Loss: 1.6608\n",
            "Epoch: 490/512... Loss: 1.6806... Val Loss: 1.6597\n",
            "Epoch: 491/512... Loss: 1.6828... Val Loss: 1.6609\n",
            "Epoch: 492/512... Loss: 1.6765... Val Loss: 1.6606\n",
            "Epoch: 493/512... Loss: 1.6901... Val Loss: 1.6593\n",
            "Epoch: 494/512... Loss: 1.6713... Val Loss: 1.6592\n",
            "Epoch: 495/512... Loss: 1.6821... Val Loss: 1.6586\n",
            "Epoch: 496/512... Loss: 1.6846... Val Loss: 1.6604\n",
            "Epoch: 497/512... Loss: 1.7045... Val Loss: 1.6682\n",
            "Epoch: 498/512... Loss: 1.6818... Val Loss: 1.6606\n",
            "Epoch: 499/512... Loss: 1.6893... Val Loss: 1.6589\n",
            "Epoch: 500/512... Loss: 1.6784... Val Loss: 1.6608\n",
            "Epoch: 501/512... Loss: 1.6842... Val Loss: 1.6599\n",
            "Epoch: 502/512... Loss: 1.6869... Val Loss: 1.6587\n",
            "Epoch: 503/512... Loss: 1.6686... Val Loss: 1.6611\n",
            "Epoch: 504/512... Loss: 1.6718... Val Loss: 1.6599\n",
            "Epoch: 505/512... Loss: 1.6781... Val Loss: 1.6622\n",
            "Epoch: 506/512... Loss: 1.6819... Val Loss: 1.6618\n",
            "Epoch: 507/512... Loss: 1.6936... Val Loss: 1.6596\n",
            "Epoch: 508/512... Loss: 1.6741... Val Loss: 1.6581\n",
            "Epoch: 509/512... Loss: 1.6697... Val Loss: 1.6590\n",
            "Epoch: 510/512... Loss: 1.6806... Val Loss: 1.6581\n",
            "Epoch: 511/512... Loss: 1.6817... Val Loss: 1.6597\n",
            "Epoch: 512/512... Loss: 1.6775... Val Loss: 1.6594\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_time: 11384.543740051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XafsPHt_AcjK"
      },
      "source": [
        "def one_hot_encode(arr, n_chars):\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_chars), dtype=np.float32)\n",
        "    # Fill the appropriate elements with ones via NumPy syntax\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_chars))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bYM-RiNJpC8f"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        \"\"\"Given a character, predict the next character.\n",
        "        Returns the predicted character and the hidden state.\n",
        "        \"\"\"\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        inputs = inputs.to(device)\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        # GG: Just for speed? Following Udacity here. Not like we are backpropagating.\n",
        "        h = h.detach()\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).detach().to(device)\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            p, top_ch = p.to('cpu'), top_ch.to('cpu')\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a66uqja2pC8g"
      },
      "source": [
        "### Priming and generating text \n",
        "\n",
        "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GivQfW8YpC8g"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "    net.to(device)\n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9NVGcggFpC8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f95018a-6d69-41bf-f81a-a1740d545cf0"
      },
      "source": [
        "print(sample(trained_model, 1000, prime='The', top_k=5))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thell betining intrest sole on the acteacher, and stone of the princh, is sort. [R.] \"The flats on the substance, or and proportude; a pelleated by stor are of was a perform three to\n",
            "sender of a chuss way in the pore.\n",
            "\n",
            "2. (Bot.) See Supposed to the particular sorged in a pains at a body and sunder in country foor to be a strengh in the classed to the charge of\n",
            "an ease; crees or through the pecumitation, and substable as an expressing in the coulse.\n",
            "\n",
            "PONTHONE\n",
            "Pal\"ment, n.\n",
            "\n",
            "Defn: A polided of a limitatinn in an anches in compouding from which an induted in to state of sometimes of chubble in the couscer, or common a book. Spenser.\n",
            "\n",
            "CHALESS\n",
            "Si\"le, v. i. [imp. & p. p.\n",
            "\n",
            "Defn: Anything some part of color; contract to the colulity in company; to rish carry so propering a capabil of an act and\n",
            "almost, a concoun antilers; the fool and the commonticul, as insects, when\n",
            "the cancel the supplied out. The pation at the\n",
            "person whom the state of\n",
            "the pass or transtrine a cans of a conceis or clothed or the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51OBTkVVuYXJ"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}